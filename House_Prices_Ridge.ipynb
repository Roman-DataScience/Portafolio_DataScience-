{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418a0b2-1c1f-44d0-b01c-1ae157606cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto de datos de entrenamiento \n",
    "df_house = pd.read_csv('train.csv')\n",
    "\n",
    "# Inspección inicial: ver las primeras filas para entender las columnas\n",
    "print(\"--- Primeras 5 Filas del Dataset ---\")\n",
    "print(df_house.head())\n",
    "\n",
    "# Inspección de información: ver tipos de datos y el gran desafío de nulos\n",
    "print(\"\\n--- Información General y Conteo de Nulos ---\")\n",
    "df_house.info()\n",
    "\n",
    "# Contar el total de valores nulos por columna\n",
    "nulos_totales = df_house.isnull().sum()\n",
    "\n",
    "# Filtrar solo las columnas que tienen al menos un valor nulo y calcular el porcentaje\n",
    "nulos_conteo = nulos_totales[nulos_totales > 0].sort_values(ascending=False)\n",
    "nulos_porcentaje = (nulos_conteo / len(df_house)) * 100\n",
    "\n",
    "# Crear un DataFrame para mostrar el resultado\n",
    "df_nulos = pd.DataFrame({\n",
    "    'Total Nulos': nulos_conteo,\n",
    "    'Porcentaje (%)': nulos_porcentaje.round(2)\n",
    "})\n",
    "\n",
    "# Imprimir el resultado para ver el ranking\n",
    "print(\"\\n--- Columnas con Valores Nulos (NaN) y su Porcentaje ---\")\n",
    "print(df_nulos)\n",
    "\n",
    "# Lista de columnas categóricas donde NaN significa 'None' (Ausencia de la característica)\n",
    "cols_none_imputation = [\n",
    "    'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n",
    "    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "    'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual',\n",
    "    'MasVnrType', 'MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd',\n",
    "    'KitchenQual', 'Functional', 'SaleType', 'Electrical', 'Foundation',\n",
    "    'Heating', 'HeatingQC', 'CentralAir', 'PavedDrive'\n",
    "]\n",
    "\n",
    "# Rellenamos los NaNs con la cadena 'None'\n",
    "for col in cols_none_imputation:\n",
    "    if col in df_house.columns:\n",
    "        df_house[col] = df_house[col].fillna('None')\n",
    "\n",
    "# Verificamos si logramos eliminar los nulos más grandes\n",
    "print(\"\\n--- Verificación: Top 5 de Nulos después de la imputación 'None' ---\")\n",
    "print(df_house.isnull().sum().sort_values(ascending=False).head())\n",
    "\n",
    "# Lista de columnas numéricas que aún tienen NaNs\n",
    "cols_median_imputation = ['LotFrontage', 'GarageYrBlt', 'MasVnrArea']\n",
    "\n",
    "# Rellenamos los NaNs con la mediana de cada columna\n",
    "for col in cols_median_imputation:\n",
    "    mediana = df_house[col].median()\n",
    "    df_house[col] = df_house[col].fillna(mediana)\n",
    "\n",
    "# Finalmente, rellenamos el único valor nulo que queda en 'Electrical' con la moda\n",
    "df_house['Electrical'] = df_house['Electrical'].fillna(df_house['Electrical'].mode()[0])\n",
    "\n",
    "# Verificación final: contamos cuántos nulos quedan en todo el DataFrame\n",
    "total_nulos_final = df_house.isnull().sum().sum()\n",
    "\n",
    "print(\"\\n--- ¡Verificación Final de Nulos! ---\")\n",
    "print(f\"Total de valores nulos restantes en todo el DataFrame: {total_nulos_final}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1. Aplicar la Transformación Logarítmica a la Variable Objetivo\n",
    "# Esto reduce el sesgo y mejora la linealidad para la regresión.\n",
    "# Se usa np.log1p (log(1+x)) para manejar el valor 0 de forma segura.\n",
    "df_house['SalePrice_Log'] = np.log1p(df_house['SalePrice'])\n",
    "\n",
    "# 2. Eliminar la variable original 'SalePrice' y el identificador 'Id'\n",
    "df_model = df_house.drop(['SalePrice', 'Id'], axis=1)\n",
    "\n",
    "# 3. Conversión de Variables Categóricas (Dummies)\n",
    "# Convertimos todas las columnas restantes de tipo 'object' (texto) a números.\n",
    "df_model = pd.get_dummies(df_model)\n",
    "\n",
    "# 4. Verificación de la Matriz Final\n",
    "print(\"\\n--- Verificación Final de la Matriz para el Modelo ---\")\n",
    "print(f\"Número de filas: {df_model.shape[0]}\")\n",
    "print(f\"Número de columnas (Features + Target): {df_model.shape[1]}\")\n",
    "print(f\"Tipos de datos únicos restantes: {df_model.dtypes.apply(lambda x: x.name).unique()}\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Definición de X e Y\n",
    "Y = df_model['SalePrice_Log'] # Variable Objetivo transformada\n",
    "# X son todas las demás columnas, excepto la variable objetivo\n",
    "X = df_model.drop('SalePrice_Log', axis=1)\n",
    "\n",
    "# 2. División de Datos (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Entrenar el Modelo de Regresión Ridge\n",
    "print(\"\\nEntrenando Modelo de Regresión Ridge...\")\n",
    "# Alpha es el término de penalización (regularización). Usamos un valor inicial.\n",
    "modelo_ridge = Ridge(alpha=10, random_state=42)\n",
    "modelo_ridge.fit(X_train, Y_train)\n",
    "\n",
    "# 4. Evaluación del Modelo (MSE)\n",
    "Y_pred = modelo_ridge.predict(X_test)\n",
    "# Usamos RMSE (Root Mean Squared Error) para evaluar la calidad de la predicción\n",
    "# Lo elevamos al cuadrado para devolverlo a la escala original del logaritmo (exponente)\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "print(f\"\\n✅ ¡RESULTADO FINAL! RMSE del modelo (en escala logarítmica): {rmse:.4f}\")\n",
    "\n",
    "# Importamos la librería que necesitamos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Creamos un DataFrame para los coeficientes\n",
    "# El modelo Ridge tiene el atributo .coef_\n",
    "df_coeficientes_house = pd.DataFrame({\n",
    "    'Característica': X.columns,\n",
    "    'Coeficiente': modelo_ridge.coef_\n",
    "})\n",
    "\n",
    "# Ordenamos por valor absoluto para encontrar las más influyentes\n",
    "df_coeficientes_house['Impacto Absoluto'] = abs(df_coeficientes_house['Coeficiente'])\n",
    "df_coeficientes_house = df_coeficientes_house.sort_values(by='Impacto Absoluto', ascending=False)\n",
    "\n",
    "# Imprimimos el Top 10 de características más influyentes\n",
    "print(\"\\n--- Top 10 Características Más Influyentes en el Precio ---\")\n",
    "print(df_coeficientes_house[['Característica', 'Coeficiente']].head(10).round(4))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Nota: Usamos el DataFrame 'df_coeficientes_house' creado en el paso anterior.\n",
    "# Necesitamos solo las 10 filas principales (las más influyentes).\n",
    "df_top_10 = df_coeficientes_house.head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Usamos un gráfico de barras horizontal (barplot)\n",
    "# La Característica va en el eje Y y el Coeficiente (el valor de impacto) en el eje X\n",
    "sns.barplot(x='Coeficiente', y='Característica', data=df_top_10, palette='RdBu')\n",
    "\n",
    "plt.title('Top 10 Coeficientes (Impacto) en el Precio de Venta (Regresión Ridge)')\n",
    "plt.xlabel('Coeficiente (Valor Positivo = Sube el Precio / Negativo = Baja el Precio)')\n",
    "plt.ylabel('Característica')\n",
    "plt.grid(axis='x', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Definir la cuadrícula de valores a probar (Grid) para el parámetro 'alpha'\n",
    "# Estos valores representan diferentes fuerzas de regularización.\n",
    "parametros = {'alpha': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0, 200.0, 500.0]}\n",
    "\n",
    "# 1. Configurar GridSearchCV\n",
    "# Usamos el modelo Ridge y definimos que el Grid Search debe buscar el 'neg_mean_squared_error' (error cuadrático medio negativo).\n",
    "# El 'neg' se usa porque Grid Search siempre busca maximizar (y queremos minimizar el error, por lo que maximizar su negativo).\n",
    "# cv=5 indica 5-fold cross-validation.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=Ridge(random_state=42),\n",
    "    param_grid=parametros,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # Usa todos los núcleos del procesador\n",
    ")\n",
    "\n",
    "# 2. Ejecutar el Grid Search (entrenamiento)\n",
    "print(\"\\nIniciando Grid Search para encontrar el alpha óptimo...\")\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# 3. Obtener el mejor resultado\n",
    "mejor_alpha = grid_search.best_params_['alpha']\n",
    "mejor_score = np.sqrt(-grid_search.best_score_) # Convertir el score negativo a RMSE positivo\n",
    "\n",
    "print(\"\\n--- Resultados de la Optimización ---\")\n",
    "print(f\"Alpha óptimo encontrado por Grid Search: {mejor_alpha}\")\n",
    "print(f\"Mejor RMSE de Validación Cruzada (con el alpha óptimo): {mejor_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3] *",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
